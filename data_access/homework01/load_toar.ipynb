{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c887f81d-618d-4bdf-ad09-a1190d77b527",
   "metadata": {},
   "source": [
    "Nils Hornstein, 7369566"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b4b24-2f70-4d74-b11e-10eab8eacba1",
   "metadata": {},
   "source": [
    "# TOAR REST-API Download Script\n",
    "This Jupyter notebook contains download scripts to download data from the TOAR database. It is assumed that a logged-in user exists for TOAR, as this is required for the script to function properly. Such an user can be created via the TOAR dashboard at the following link: https://toar-data.fz-juelich.de/gui/v2/auth/logout=True.\n",
    "<br>The notebook contains three different setups to download data from TOAR:\n",
    "1. Via the TOAR REST API analysis service and data saved in JSON files\n",
    "2. Via the TOAR REST API analysis service and data saved in csv files\n",
    "3. Via the TOAR REST API time series endpoint and data saved in csv files\n",
    "\n",
    "Each script creates its own folder structure to store the downloaded data and downloads the same measurement data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1487,
   "id": "6192d8b3-a1df-44ec-9297-30bf28d03d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28607c31-b440-4bf1-a5cc-fd0bb6829d02",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "Insert your access token that can be created in your user profile. This is the only modification you have to do in order to run the download scripts.\n",
    "<br>***Note!** Your access token will be only valid for one hour after creation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1520,
   "id": "67154bfe-fa17-4b29-aa76-a97ab4480866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ACCESS_TOKEN = 'YOUR_ACCESS_TOKEN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31848e2f-ae97-4bc4-9991-77dab51e7281",
   "metadata": {},
   "source": [
    "These arguments are given to each download script. As a result, each variant downloads the same data. This data consists of measurements of the variable 'particles up to 10 µm diameter' from a station in Cologne, with a specific flag and in the period from 1 December 2023 to 7 December 2023. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1500,
   "id": "ce007e53-86ea-4f5e-9490-6d69c16b765a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TOAR_SERVICE_URL=\"https://toar-data.fz-juelich.de/api/v2/\"\n",
    "headers = {'AccessToken': ACCESS_TOKEN}\n",
    "flags = \"AllOK\"\n",
    "start_date = \"2023-12-01T00:00:00\"\n",
    "end_date = \"2023-12-07T23:59:59\"\n",
    "date_range = f\"{start_date},{end_date}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb782d-0dfd-4752-814d-7a34fb11f6a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data download via TOAR analysis service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1518,
   "id": "ef40b211-8fba-4bea-8924-169a70f8c68b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send request to REST API ...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Send request to REST API ...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Request the data of one time series on a specific variable, from a specific station, in a specific date range, and specific quality flags  \n",
    "resp = requests.get( \n",
    "    f\"{TOAR_SERVICE_URL}analysis/data/timeseries/\"\n",
    "    \"?station_id=37,38,403,404\"\n",
    "    \"&data_origin=Instrument\"\n",
    "    \"&data_origin_type=Measurement\"\n",
    "    \"&variable_id=13\"\n",
    "    f\"&flags={flags}\"\n",
    "    f\"&daterange={date_range}\"\n",
    "    \"&limit=1\", \n",
    "    headers=headers,\n",
    "    timeout=(3.05, 20) \n",
    ")\n",
    "\n",
    "result_url = resp.json().get('status', None)  # None, if task id does not exist\n",
    "# print(result_url) # Control output of the task ID and URL\n",
    "\n",
    "# Downlaod the file from our request as a ZIP archive\n",
    "start_download_time = time.time()\n",
    "resp = requests.get(result_url)\n",
    "zip_content = BytesIO(resp.content)\n",
    "end_download_time = time.time()\n",
    "download_time = end_download_time - start_download_time\n",
    "print(f\"Download finished in {download_time:.2f} seconds.\")\n",
    "\n",
    "with zipfile.ZipFile(zip_content, \"r\") as zip_file:\n",
    "    json_name = zip_file.namelist()[0]\n",
    "    with zip_file.open(json_name) as file:\n",
    "        content = file.read().decode(\"utf-8\").strip()\n",
    "        json_text = \"\\n\".join(line for line in content.splitlines() if not line.startswith(\"#\")) # delete first line of the JSON file which has no usage\n",
    "        data = json.loads(json_text)\n",
    "        \n",
    "all_data = data.get(\"data\", [])\n",
    "\n",
    "# Get metadata to name folder dynamically\n",
    "station_id = data.get(\"metadata\", {}).get(\"station\", {}).get(\"id\", \"unknown\")\n",
    "station_name = data.get(\"metadata\", {}).get(\"station\", {}).get(\"name\", \"unknown\")\n",
    "variable_name = data.get(\"metadata\", {}).get(\"variable\", {}).get(\"name\", \"unknown\")\n",
    "        \n",
    "# Create dataframe \n",
    "df = pd.DataFrame(all_data)\n",
    "if not df.empty and 'datetime' in df.columns:\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "    df.set_index('datetime', inplace=True)\n",
    "\n",
    "print(df.head())\n",
    "print(\"Number of rows:\", len(df))\n",
    "\n",
    "# Save data as JSON files for each day\n",
    "if not df.empty:\n",
    "    for day, daily_data in df.groupby(pd.Grouper(freq='D')):\n",
    "        if daily_data.empty:\n",
    "            continue\n",
    "            \n",
    "        folder_path = os.path.join(\n",
    "            \"TOAR Data via Analysis Service in JSON\",\n",
    "            f\"Station {station_id}, {station_name}\",\n",
    "            f\"Variable {variable_name}\",\n",
    "            str(day.year),\n",
    "            calendar.month_name[day.month]\n",
    "        )\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        filepath = os.path.join(folder_path, f\"{day.date()}.json\")\n",
    "        daily_json = (\n",
    "            daily_data.reset_index()\n",
    "            .assign(datetime=lambda df: df[\"datetime\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\"))\n",
    "            .to_dict(orient=\"records\")\n",
    "        )\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(daily_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"{filepath} saved. Rows: {len(daily_data)}\")\n",
    "        \n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Whole process finished in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1479,
   "id": "e5b99d12-816f-43b3-a68c-b3dba0fd958c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send request to REST API ...\n"
     ]
    },
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1479], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#print(status_url) # Control output of the task ID and URL\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Downlaod the file from our request as a ZIP archive\u001b[39;00m\n\u001b[0;32m     22\u001b[0m start_download_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 23\u001b[0m resp \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(result_url)\n\u001b[0;32m     24\u001b[0m zip_content \u001b[38;5;241m=\u001b[39m BytesIO(resp\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m     25\u001b[0m end_download_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[0;32m    563\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(\n\u001b[0;32m    564\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    565\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(req)\n\u001b[0;32m    577\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    579\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_environment_settings(\n\u001b[0;32m    580\u001b[0m     prep\u001b[38;5;241m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[0;32m    581\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:484\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    481\u001b[0m     auth \u001b[38;5;241m=\u001b[39m get_netrc_auth(request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    483\u001b[0m p \u001b[38;5;241m=\u001b[39m PreparedRequest()\n\u001b[1;32m--> 484\u001b[0m p\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[0;32m    485\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m    486\u001b[0m     url\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m    487\u001b[0m     files\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mfiles,\n\u001b[0;32m    488\u001b[0m     data\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mdata,\n\u001b[0;32m    489\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mjson,\n\u001b[0;32m    490\u001b[0m     headers\u001b[38;5;241m=\u001b[39mmerge_setting(\n\u001b[0;32m    491\u001b[0m         request\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, dict_class\u001b[38;5;241m=\u001b[39mCaseInsensitiveDict\n\u001b[0;32m    492\u001b[0m     ),\n\u001b[0;32m    493\u001b[0m     params\u001b[38;5;241m=\u001b[39mmerge_setting(request\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams),\n\u001b[0;32m    494\u001b[0m     auth\u001b[38;5;241m=\u001b[39mmerge_setting(auth, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth),\n\u001b[0;32m    495\u001b[0m     cookies\u001b[38;5;241m=\u001b[39mmerged_cookies,\n\u001b[0;32m    496\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mmerge_hooks(request\u001b[38;5;241m.\u001b[39mhooks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks),\n\u001b[0;32m    497\u001b[0m )\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:367\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_method(method)\n\u001b[1;32m--> 367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_url(url, params)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_headers(headers)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_cookies(cookies)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:438\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[38;5;241m*\u001b[39me\u001b[38;5;241m.\u001b[39margs)\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheme:\n\u001b[1;32m--> 438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingSchema(\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No scheme supplied. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerhaps you meant https://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m     )\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m host:\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No host supplied\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?"
     ]
    }
   ],
   "source": [
    "print(f\"Send request to REST API ...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Request the data of one time series on a specific variable, from a specific station, in a specific date range, and specific quality flags  \n",
    "resp = requests.get( \n",
    "    f\"{TOAR_SERVICE_URL}analysis/data/timeseries/\"\n",
    "    \"?station_id=37,38,403,404\"\n",
    "    \"&data_origin=Instrument\"\n",
    "    \"&data_origin_type=Measurement\"\n",
    "    \"&variable_id=13\"\n",
    "    f\"&flags={flags}\"\n",
    "    f\"&daterange={date_range}\"\n",
    "    \"&limit=1\", \n",
    "    headers=headers,\n",
    "    timeout=(3.05, 20) \n",
    ")\n",
    "\n",
    "result_url = resp.json().get('status', None)  # None, if task id does not exist\n",
    "#print(status_url) # Control output of the task ID and URL\n",
    "\n",
    "# Downlaod the file from our request as a ZIP archive\n",
    "start_download_time = time.time()\n",
    "resp = requests.get(result_url)\n",
    "zip_content = BytesIO(resp.content)\n",
    "end_download_time = time.time()\n",
    "download_time = end_download_time - start_download_time\n",
    "print(f\"Download finished in {download_time:.2f} seconds.\")\n",
    "\n",
    "with zipfile.ZipFile(zip_content, \"r\") as zip_file:\n",
    "    json_name = zip_file.namelist()[0]\n",
    "    with zip_file.open(json_name) as file:\n",
    "        content = file.read().decode(\"utf-8\").strip()\n",
    "        json_text = \"\\n\".join(line for line in content.splitlines() if not line.startswith(\"#\")) # delete first line of the JSON file which has no usage\n",
    "        data = json.loads(json_text)\n",
    "        \n",
    "all_data = data.get(\"data\", [])\n",
    "\n",
    "# Get metadata to name folder dynamically\n",
    "station_id = data.get(\"metadata\", {}).get(\"station\", {}).get(\"id\", \"unknown\")\n",
    "station_name = data.get(\"metadata\", {}).get(\"station\", {}).get(\"name\", \"unknown\")\n",
    "variable_name = data.get(\"metadata\", {}).get(\"variable\", {}).get(\"name\", \"unknown\")\n",
    "        \n",
    "# Create dataframe \n",
    "df = pd.DataFrame(all_data)\n",
    "if not df.empty and 'datetime' in df.columns:\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "    df.set_index('datetime', inplace=True)\n",
    "\n",
    "print(df.head())\n",
    "print(\"Number of rows:\", len(df))\n",
    "\n",
    "# Save data as CSV files for each day\n",
    "if not df.empty:\n",
    "    start_date = df.index.min().normalize()\n",
    "    end_date = df.index.max().normalize()\n",
    "    current_day = start_date\n",
    "\n",
    "    while current_day <= end_date:\n",
    "        next_day = current_day + timedelta(days=1)\n",
    "        daily_data = df[(df.index >= current_day) & (df.index < next_day)]\n",
    "        \n",
    "        if not daily_data.empty:\n",
    "            year = str(current_day.year)\n",
    "            month_name = calendar.month_name[current_day.month]\n",
    "            folder_path = os.path.join(\n",
    "                \"TOAR Data via Analysis Service in csv\",\n",
    "                f\"Station_{station_id or 'unknown'}\",\n",
    "                f\"Variable_{variable_name or 'unknown'}\",\n",
    "                year,\n",
    "                month_name\n",
    "            )\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            \n",
    "            filename = f\"{current_day.date()}.csv\"\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            daily_data.to_csv(filepath, encoding=\"utf-8\")\n",
    "            print(f\"{filepath} saved. Rows: {len(daily_data)}\")\n",
    "        \n",
    "        current_day = next_day\n",
    "        \n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Whole process finished in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb6396-53df-4e5a-9271-a7e2f56d79a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data download via time series endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51996e94-23e3-44f2-973f-0d6aff465652",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites (find your time series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1526,
   "id": "431e3156-4646-4f5c-a2e6-18a52e19cabc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19956, 19958, 19986, 19987, 31457, 31544, 31830, 31866, 31869, 31887, 31901, 32153, 32160]\n"
     ]
    }
   ],
   "source": [
    "# Find possible time series we want to download according to specified criteria via search service\n",
    "resp = requests.get( \n",
    "    f\"{TOAR_SERVICE_URL}/search/\"\n",
    "    \"?station_id=37,38,403,404\"\n",
    "    \"&data_origin=Instrument\"\n",
    "    \"&data_origin_type=Measurement\"\n",
    "    \"&variable_id=13\"\n",
    "    \"&limit=None\", \n",
    "    headers=headers,\n",
    "    timeout=(3.05, 20) \n",
    ")\n",
    "time_series = resp.json()\n",
    "time_series_ids = [ts.get(\"id\") for ts in time_series]\n",
    "print(time_series_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1528,
   "id": "24d59ef2-7e03-4477-a25b-8de0a73f84a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 19956, 'label': '', 'order': 1, 'sampling_frequency': 'hourly', 'aggregation': 'mean of two values', 'data_start_date': '2012-01-01T00:00:00+00:00', 'data_end_date': '2025-12-03T21:00:00+00:00', 'data_origin': 'instrument', 'data_origin_type': 'measurement', 'provider_version': 'N/A', 'sampling_height': 2.0, 'additional_metadata': {}, 'doi': '', 'coverage': -1.0, 'station': {'id': 37, 'codes': ['DENW053'], 'name': 'Köln-Chorweiler', 'coordinates': {'lat': 51.019345, 'lng': 6.884636, 'alt': 45.0}, 'coordinate_validation_status': 'not checked', 'country': 'Germany', 'state': 'Nordrhein-Westfalen', 'type': 'background', 'type_of_area': 'urban', 'timezone': 'Europe/Berlin', 'additional_metadata': {'rice_production': 0.0, 'station_alt_flag': '0', 'wheat_production': 0.0, 'google_resolution': '153', 'soybean_production': 0.0, 'station_google_alt': '46', 'station_reported_alt': '45', 'station_landcover_description': 'Urbanandbuilt-up:53.9%,Croplands:20.3%,Mixedforest:17.4%,Cropland/Naturalvegetationmosaic:5.7%,EvergreenNeedleleafforest:2.1%', 'station_max_population_density_5km': '31605'}, 'aux_images': [], 'aux_docs': [], 'aux_urls': [], 'globalmeta': {'mean_topography_srtm_alt_90m_year1994': 46.0, 'mean_topography_srtm_alt_1km_year1994': 46.59005145797599, 'max_topography_srtm_relative_alt_5km_year1994': 19.0, 'min_topography_srtm_relative_alt_5km_year1994': -15.0, 'stddev_topography_srtm_relative_alt_5km_year1994': -41.33330258839917, 'climatic_zone_year2016': '5 (warm temperate moist)', 'htap_region_tier1_year2010': '4 (EUR Western + Eastern EU+Turkey (upto 66 N polar circle))', 'dominant_landcover_year2012': '190 (Urban areas)', 'landcover_description_25km_year2012': '190 (Urban areas): 33.5 %, 11 (Cropland, rainfed, herbaceous cover): 32.3 %, 60 (Tree cover, broadleaved, deciduous, closed to open (>15%)): 7.7 %, 100 (Mosaic tree and shrub (>50%) / herbaceous cover (<50%)): 5.4 %, 130 (Grassland): 5.0 %, 90 (Tree cover, mixed leaf type (broadleaved and needleleaved)): 3.2 %, 10 (Cropland, rainfed): 2.8 %, 30 (Mosaic cropland (>50%) / natural vegetation (tree, shrub, herbaceous cover) (<50%)): 2.7 %, 40 (Mosaic natural vegetation (tree, shrub, herbaceous cover) (>50%) / cropland (<50%)): 2.7 %, 210 (Water bodies): 2.7 %, 70 (Tree cover, needleleaved, evergreen, closed to open (>15%)): 1.7 %', 'dominant_ecoregion_year2017': '664 (European Atlantic mixed forests)', 'ecoregion_description_25km_year2017': '664 (European Atlantic mixed forests): 73.2 %, 686 (Western European broadleaf forests): 26.8 %', 'distance_to_major_road_year2020': 19.296804133940753, 'mean_stable_nightlights_1km_year2013': 56.0, 'mean_stable_nightlights_5km_year2013': 52.965986394557824, 'max_stable_nightlights_25km_year2013': 63.0, 'max_stable_nightlights_25km_year1992': 62.0, 'mean_population_density_250m_year2015': 4558.702793282464, 'mean_population_density_5km_year2015': 1755.4906153516285, 'max_population_density_25km_year2015': 6384.33743659605, 'mean_population_density_250m_year1990': 2946.8366593932205, 'mean_population_density_5km_year1990': 1299.1245131476696, 'max_population_density_25km_year1990': 5977.276754990633, 'mean_nox_emissions_10km_year2015': 40544.69921875, 'mean_nox_emissions_10km_year2000': 28117.1640625, 'toar1_category': 'unclassified', 'toar2_category': 'urban'}, 'changelog': [{'datetime': '2025-02-16T18:32:04.363810+00:00', 'description': 'add toar2_category from ML application', 'old_value': \"{'toar2_category': 'Unknown'}\", 'new_value': \"{'toar2_category': 'Urban'}\", 'station_id': 37, 'author_id': 1, 'type_of_change': 'single value correction in metadata'}, {'datetime': '2025-06-25T11:38:44.903275+00:00', 'description': 'Sabine Schröder (s.schroeder@fz-juelich.de): NOLOG', 'old_value': '{\\'additional_metadata\\': \\'{\"station_alt_flag\": \"0\", \"google_resolution\": \"153\", \"station_google_alt\": \"46\", \"station_reported_alt\": \"45\", \"station_landcover_description\": \"Urbanandbuilt-up:53.9%,Croplands:20.3%,Mixedforest:17.4%,Cropland/Naturalvegetationmosaic:5.7%,EvergreenNeedleleafforest:2.1%\", \"station_max_population_density_5km\": \"31605\"}\\'}', 'new_value': \"{'additional_metadata': {'station_alt_flag': '0', 'google_resolution': '153', 'station_google_alt': '46', 'station_reported_alt': '45', 'station_landcover_description': 'Urbanandbuilt-up:53.9%,Croplands:20.3%,Mixedforest:17.4%,Cropland/Naturalvegetationmosaic:5.7%,EvergreenNeedleleafforest:2.1%', 'station_max_population_density_5km': '31605', 'rice_production': 0.0, 'soybean_production': 0.0, 'wheat_production': 0.0}}\", 'station_id': 37, 'author_id': 17, 'type_of_change': 'single value correction in metadata'}]}, 'variable': {'name': 'pm10', 'longname': 'particles up to 10 µm diameter', 'displayname': 'PM 10', 'cf_standardname': 'mass_concentration_of_pm10_ambient_aerosol_in_air', 'units': 'µg m-3', 'chemical_formula': '', 'id': 13}, 'programme': {'id': 0, 'name': 'None', 'longname': '', 'homepage': '', 'description': ''}, 'roles': [{'id': 1, 'role': 'resource provider', 'status': 'active', 'contact': {'id': 39, 'organisation': {'id': 35, 'name': 'UBA', 'longname': 'Umweltbundesamt', 'kind': 'government', 'city': '', 'postcode': '', 'street_address': '', 'country': 'Germany', 'homepage': '', 'contact_url': 'mailto:immission@uba.de'}}}, {'id': 21, 'role': 'contributor', 'status': 'active', 'contact': {'id': 40, 'organisation': {'id': 36, 'name': 'LANUV', 'longname': 'Landesamt für Natur, Umwelt und Verbraucherschutz Nordrhein-Westfalen', 'kind': 'government', 'city': '', 'postcode': '', 'street_address': '', 'country': 'Germany', 'homepage': '', 'contact_url': ''}}}], 'changelog': [{'datetime': '2021-11-05T18:26:55.061591+00:00', 'description': 'time series created', 'old_value': '', 'new_value': '', 'timeseries_id': 19956, 'author_id': 1, 'type_of_change': 'created'}, {'datetime': '2022-02-06T17:05:23.529181+00:00', 'description': 'add available timerange', 'old_value': \"{'data_start_date': '1900-01-01 00:00:00+01:00', 'data_end_date': '1900-01-01 00:00:00+01:00'}\", 'new_value': \"{'data_start_date': datetime.datetime(2012, 1, 1, 1, 0, tzinfo=datetime.timezone.utc), 'data_end_date': datetime.datetime(2021, 11, 30, 14, 0, tzinfo=datetime.timezone.utc)}\", 'timeseries_id': 19956, 'author_id': 1, 'type_of_change': 'comprehensive metadata revision'}]}\n",
      "19956\n"
     ]
    }
   ],
   "source": [
    "# Find a time series we want to download via timeseries endpoint search that is the first one that fulfil further specifications\n",
    "resp = requests.get( \n",
    "    f\"{TOAR_SERVICE_URL}/timeseries/{ids[0]}\"\n",
    "    f\"?flags={flags}\"\n",
    "    f\"&daterange={date_range}\"\n",
    "    \"&limit=1\", \n",
    "    headers=headers,\n",
    "    timeout=(3.05, 20) \n",
    ")\n",
    "ts = resp.json()\n",
    "ts_id = ts.get(\"id\")\n",
    "print(ts)\n",
    "print(ts_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff21d03-e704-48db-ba36-e96cff518f82",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Date arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1530,
   "id": "bba41832-dfc2-4329-81a5-3521f8f07f27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the download of time series 19956 ...\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1530], line 11\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Download the time series we have found before\u001b[39;00m\n\u001b[0;32m      5\u001b[0m result \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTOAR_SERVICE_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/data/timeseries/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mts_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?format=csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#headers=headers,\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3.05\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m timeseries_meta \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([line[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m)]))\n\u001b[0;32m     13\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     14\u001b[0m     StringIO(result\u001b[38;5;241m.\u001b[39mtext),\n\u001b[0;32m     15\u001b[0m     comment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     17\u001b[0m     index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Make sure the index is timezone-aware in UTC\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "print(f\"Start the download of time series {ts_id} ...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Download the time series we have found before\n",
    "result = requests.get(\n",
    "    f\"{TOAR_SERVICE_URL}/data/timeseries/{ts_id}?format=csv\",\n",
    "    headers=headers,\n",
    "    timeout=(3.05, 10)\n",
    ")\n",
    "\n",
    "timeseries_meta = json.loads(\"\\n\".join([line[1:] for line in result.text.split('\\n') if line.startswith('#')]))\n",
    "\n",
    "df = pd.read_csv(\n",
    "    StringIO(result.text),\n",
    "    comment=\"#\",\n",
    "    parse_dates=[\"datetime\"],\n",
    "    index_col=\"datetime\"\n",
    ")\n",
    "\n",
    "# Make sure the index is timezone-aware in UTC\n",
    "if df.index.tz is None:\n",
    "    df.index = df.index.tz_localize(\"UTC\")\n",
    "else:\n",
    "    df.index = df.index.tz_convert(\"UTC\")\n",
    "\n",
    "# define range\n",
    "if not isinstance(start_date, pd.Timestamp):\n",
    "    start_date = pd.to_datetime(start_date, utc=True)\n",
    "if not isinstance(end_date, pd.Timestamp):\n",
    "    end_date = pd.to_datetime(end_date, utc=True)\n",
    "\n",
    "# Get metadata to name folder dynamically\n",
    "station_id = timeseries_meta.get(\"station\", {}).get(\"id\", \"unknown\")\n",
    "station_name = timeseries_meta.get(\"station\", {}).get(\"name\", \"unknown\")\n",
    "variable_name = timeseries_meta.get(\"variable\", {}).get(\"name\", \"unknown\")\n",
    "\n",
    "# Create a folder to store the csv files\n",
    "folder_path = os.path.join(\n",
    "    \"TOAR Data via time series endpoint in csv\",\n",
    "    f\"Station {station_id}, {station_name}\",\n",
    "    f\"Variable {variable_name}\",\n",
    "    str(day.year),\n",
    "    calendar.month_name[day.month]\n",
    ")\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "#  Iterate over each day in time range and save each as a csv file\n",
    "current_day = start_date\n",
    "\n",
    "while current_day <= end_date:\n",
    "    next_day = current_day + timedelta(days=1)\n",
    "\n",
    "    daily_data = df[(df.index >= current_day) & (df.index < next_day)]\n",
    "\n",
    "    if not daily_data.empty:\n",
    "        filename = f\"{current_day.date()}.csv\"\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        daily_data.to_csv(filepath, encoding=\"utf-8\")\n",
    "        print(f\"{filename} saved. Rows: {len(daily_data)}\")\n",
    "\n",
    "    current_day = next_day\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Whole process finished in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927444e4-fefc-4c24-8d80-00c41c12b9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
